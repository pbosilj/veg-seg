{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d89f2d",
   "metadata": {},
   "source": [
    "### Demo of Crop_Dataset dataloader\n",
    "\n",
    "Should (eventually) support loading `CA17`, `ON17` (both partial and full annotations) and `SB16` datasets.\n",
    "\n",
    "For now has only been tested on `CA17` with full annotations (as there is structurally no difference from `ON17` data that should work too).\n",
    "\n",
    "Input `sample_size` will ensure slicing the input images to samples of given size.\n",
    "\n",
    "**TODO**:\n",
    "- partial annotations `CA17`/`ON17`\n",
    "- `SB16`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba67f4",
   "metadata": {},
   "source": [
    "Functions to collate the batches into a 4D tensors instead of tuples of 3D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d470c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_list(images, fill_value=0):\n",
    "    max_size = tuple(max(s) for s in zip(*[img.shape for img in images]))\n",
    "    batch_shape = (len(images),) + max_size\n",
    "    batched_imgs = images[0].new(*batch_shape).fill_(fill_value)\n",
    "    for img, pad_img in zip(images, batched_imgs):\n",
    "        pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)\n",
    "    return batched_imgs\n",
    "\n",
    "# Collates into a 4th tensor dimension rather than tuples:\n",
    "def collate_fn(batch):\n",
    "    images, targets = list(zip(*batch))\n",
    "    batched_imgs = cat_list(images, fill_value=0)\n",
    "    batched_targets = cat_list(targets, fill_value=255)\n",
    "    return batched_imgs, batched_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0afcb",
   "metadata": {},
   "source": [
    "Dataset settings:\n",
    "- input folder\n",
    "- `sample_size` to slice the input images\n",
    "- `samples_per_image` to control the number of input slices if less than allowed by `sample_size`\n",
    "\n",
    "**Note:** Customised transforms for segmentation-type datasets are used from `vegseg_transforms.py` (local file). These ensure that the (image, mask) pairs are correctly transformed together. (taken from a VOC example somewhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67681de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from vegseg_transforms import Normalize, Compose, Resize, ToTensor\n",
    "\n",
    "import crop_datasets\n",
    "\n",
    "#DATA_ROOT = './voc/'\n",
    "\n",
    "CARROT_ROOT = \"/home/pbosilj/Data/CA17/carrots_labelled\"\n",
    "#sample_size = (int(384/2),int(512/2))\n",
    "sample_size = (384,512)\n",
    "samples_per_image = (4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367d2e8",
   "metadata": {},
   "source": [
    "Get dataset image stats (mean and std).\n",
    "\n",
    "This is done by loading all the data at once and calculating mean and std accross all data. This is then used to set up a normalised dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144f5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'train.txt' does not exist in root. Looking for 'test.txt'\n",
      "256 images in the training set\n",
      "Dataset mean: tensor([0.6396, 0.5730, 0.4229, 0.5532]) std: tensor([0.1391, 0.1122, 0.0918, 0.1718])\n"
     ]
    }
   ],
   "source": [
    "my_transforms = ToTensor()\n",
    "\n",
    "\n",
    "\n",
    "carrots_train = crop_datasets.Crop_Dataset(root=CARROT_ROOT,\n",
    "                                           train=True,\n",
    "                                           partial_truth=True,\n",
    "                                           sample_size=sample_size,\n",
    "                                           samples_per_image=samples_per_image,\n",
    "                                           transforms=ToTensor())\n",
    "\n",
    "print(\"{} images in the training set\".format(len(carrots_train)))\n",
    "\n",
    "single_loader = torch.utils.data.DataLoader(carrots_train, batch_size=len(carrots_train), num_workers=1)\n",
    "data, labels = next(iter(single_loader))\n",
    "\n",
    "d_mean = data.mean(axis=(0,2,3))\n",
    "d_std = data.std(axis=(0,2,3))\n",
    "    \n",
    "print(\"Dataset mean: {} std: {}\".format(d_mean, d_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410dadd",
   "metadata": {},
   "source": [
    "Get datasets stats class. This can be used when setting up the loss criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9074be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probabilities: tensor([0.7844, 0.0781, 0.1374])\n"
     ]
    }
   ],
   "source": [
    "class_probs = carrots_train.get_class_probability()\n",
    "class_weights = 1.0/class_probs\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(\"Class probabilities: {}\".format(class_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406f5b9",
   "metadata": {},
   "source": [
    "Set up a normalised dataset (using mean and std calculated above). Relies on customised transforms for segmentation data implemented in `vegseg_transforms.py`.\n",
    "\n",
    "Set up a data loader which loads 4 by 4 images in a single batch (using the collate functions from above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d466d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'train.txt' does not exist in root. Looking for 'test.txt'\n"
     ]
    }
   ],
   "source": [
    "my_transform = Compose([ToTensor(), Normalize(d_mean, d_std)])\n",
    "\n",
    "carrots_train_norm = crop_datasets.Crop_Dataset(\n",
    "                    root = CARROT_ROOT,\n",
    "                    train = True,\n",
    "                    partial_truth = True,\n",
    "                    sample_size = sample_size,\n",
    "                    samples_per_image=samples_per_image,\n",
    "                    transforms = my_transforms\n",
    "                    )\n",
    "#test_sampler = torch.utils.data.SequentialSampler(carrots_test_norm)\n",
    "\n",
    "carrot_loader_train = torch.utils.data.DataLoader(\n",
    "    carrots_train_norm, batch_size=4,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b69037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "print(carrots_train_norm.get_ignore_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dc9b1",
   "metadata": {},
   "source": [
    "Test the data loader by looping through three batches and printing some basic stats. **Note:** this example is using samples of half width and length from the paper, just to test stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b82306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([4, 4, 384, 512])\n",
      "<class 'torch.Tensor'> torch.Size([4, 384, 512])\n",
      "Image  max: 1.0 min: 0.09019608050584793\n",
      "Target max: 255 min: 0\n",
      "Unique labels: [  0   1   2 255]\n",
      "\n",
      "<class 'torch.Tensor'> torch.Size([4, 4, 384, 512])\n",
      "<class 'torch.Tensor'> torch.Size([4, 384, 512])\n",
      "Image  max: 1.0 min: 0.13725490868091583\n",
      "Target max: 255 min: 0\n",
      "Unique labels: [  0   1   2 255]\n",
      "\n",
      "<class 'torch.Tensor'> torch.Size([4, 4, 384, 512])\n",
      "<class 'torch.Tensor'> torch.Size([4, 384, 512])\n",
      "Image  max: 1.0 min: 0.08235294371843338\n",
      "Target max: 255 min: 0\n",
      "Unique labels: [  0   1   2 255]\n",
      "\n",
      "<class 'torch.Tensor'> torch.Size([4, 4, 384, 512])\n",
      "<class 'torch.Tensor'> torch.Size([4, 384, 512])\n",
      "Image  max: 1.0 min: 0.07450980693101883\n",
      "Target max: 255 min: 0\n",
      "Unique labels: [  0   1   2 255]\n",
      "\n",
      "<class 'torch.Tensor'> torch.Size([4, 4, 384, 512])\n",
      "<class 'torch.Tensor'> torch.Size([4, 384, 512])\n",
      "Image  max: 1.0 min: 0.10588235408067703\n",
      "Target max: 255 min: 0\n",
      "Unique labels: [  0   1   2 255]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i, (image, target) in enumerate(carrot_loader_train):\n",
    "    print(type(image), image.size())\n",
    "    print(type(target), target.size())\n",
    "    \n",
    "    np_image = image.numpy()\n",
    "    np_target = target.numpy()\n",
    "    \n",
    "    print(\"Image  max: {} min: {}\".format(np.amax(np_image), np.amin(np_image)))\n",
    "    print(\"Target max: {} min: {}\".format(np.amax(np_target), np.amin(np_target)))\n",
    "    print(\"Unique labels: {}\".format(np.unique(np_target)))\n",
    "    print()\n",
    "    \n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af2482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
